{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Practical exercise: Australian charities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Computational methods for collecting, cleaning and analysing data are an increasingly important component of a social scientistâ€™s toolkit. Central to engaging in these methods is the ability to write readable and effective code using a programming language.\n",
    "\n",
    "In this practical we attempt to scrape information on the organisational status of Australian charities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aims\n",
    "\n",
    "This practical has one aim:\n",
    "1. Successfully scrape information relating to Australian charities' organisational status e.g., does it still operate? When was it registered?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Guide to using this resource\n",
    "\n",
    "This learning resource was built using <a href=\"https://jupyter.org/\" target=_blank>Jupyter Notebook</a>, an open-source software application that allows you to mix code, results and narrative in a single document. As <a href=\"https://jupyter4edu.github.io/jupyter-edu-book/\" target=_blank>Barba et al. (2019)</a> espouse:\n",
    "> In a world where every subject matter can have a data-supported treatment, where computational devices are omnipresent and pervasive, the union of natural language and computation creates compelling communication and learning opportunities.\n",
    "\n",
    "If you are familiar with Jupyter notebooks then skip ahead to the main content (*What is web-scraping?*). Otherwise, the following is a quick guide to navigating and interacting with the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interaction\n",
    "\n",
    "**You only need to execute the code that is contained in sections which are marked by `In []`.**\n",
    "\n",
    "To execute a cell, click or double-click the cell and press the `Run` button on the top toolbar (you can also use the keyboard shortcut Shift + Enter).\n",
    "\n",
    "Try it for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Enter your name and press enter:\")\n",
    "name = input()\n",
    "print(\"\\r\")\n",
    "print(\"Hello {}, enjoy learning more about Python and web-scraping!\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learn more\n",
    "\n",
    "Jupyter notebooks provide rich, flexible features for conducting and documenting your data analysis workflow. To learn more about additional notebook features, we recommend working through some of the <a href=\"https://github.com/darribas/gds19/blob/master/content/labs/lab_00.ipynb\" target=_blank>materials</a> provided by Dani Arribas-Bel at the University of Liverpool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is the general approach for scraping data from a web page?\n",
    "\n",
    "We begin by identifying a web page containing information we are interested in collecting. Then we need to **know** the following:\n",
    "1. The location (i.e., web address) where the web page can be accessed. For example, the UK Data Service homepage can be accessed via <a href=\"https://ukdataservice.ac.uk/\" target=_blank>https://ukdataservice.ac.uk/</a>.\n",
    "2. The location of the information we are interested in within the structure of the web page. This involves visually inspecting a web page's underlying code using a web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And **do** the following:\n",
    "1. Request the web page using its web address.\n",
    "2. Parse the structure of the web page so your programming language can work with its contents.\n",
    "3. Extract the information we are interested in.\n",
    "4. Write this information to a file for future use.\n",
    "\n",
    "For any programming task, it is useful to write out the steps needed to solve the problem: we call this *pseudo-code*, as it is captures the main tasks and the order in which they need to be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Details\n",
    "\n",
    "This is an example drawn from my (Diarmuid) own research. I am interested in the impact of COVID-19 on the foundation and dissolution charities across a number of countries. To study these phenomena I need the organisational status &mdash; foundation/dissolution date, organisational status &mdash; of individual charities. The Australian charity regulator provides high quality, open data on the organisational status of charities, with the exception of dissolution status. Therefore I wrote a script that takes a list of charity ids and scrapes information on organisational status from the regulator's website.\n",
    "\n",
    "Your task is to execute and complete sections of this web scraping script.\n",
    "\n",
    "It's a bit more complicated than what we've encountered so far, but gives you a sense of what web scraping for social research is really like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Identifying the web address\n",
    "\n",
    "An example of a charity's web page can be viewed at the following web address: https://www.acnc.gov.au/charity/3b7aa8b31249837c15657331aeb54821"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Locating information\n",
    "\n",
    "The information we need located in the *History* tab underneath the **Registration status history** heading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Visually inspecting the underlying HTML code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: inspect the web page of our example charity (https://www.acnc.gov.au/charity/3b7aa8b31249837c15657331aeb54821) and insert the relevant HTML into the cell below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table class=\"views-table cols-2 table\" >\n",
    "        <thead>\n",
    "      <tr>\n",
    "                  <th class=\"views-field views-field-field-date\" >\n",
    "            Effective Date          </th>\n",
    "                  <th class=\"views-field views-field-title\" >\n",
    "            Status          </th>\n",
    "              </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "          <tr class=\"odd views-row-first views-row-last\">\n",
    "                  <td class=\"views-field views-field-field-date\" >\n",
    "            <span class=\"date-display-single\" property=\"dc:date\" datatype=\"xsd:dateTime\" content=\"2012-12-03T00:00:00+11:00\">3 December 2012</span>          </td>\n",
    "                  <td class=\"views-field views-field-title\" >\n",
    "            Registered          </td>\n",
    "              </tr>\n",
    "      </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Requesting the web page\n",
    "\n",
    "**TASK**: import the `requests`, `csv` and `os` modules into this Python session (`datetime` and `BeautifulSoup` are already listed for you)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import csv\n",
    "from datetime import datetime # module for working with dates and time\n",
    "from bs4 import BeautifulSoup as soup # module for parsing web pages\n",
    "\n",
    "print(\"Succesfully imported necessary modules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: fill in the blanks (e.g., # INSERT URL #) with the necessary code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the URL where the web page can be accessed\n",
    "\n",
    "url = \"https://www.acnc.gov.au/charity/3b7aa8b31249837c15657331aeb54821\" # INSERT URL #\n",
    "\n",
    "\n",
    "# Request the web page from the URL\n",
    "\n",
    "response =  requests.get(url, allow_redirects = False, timeout = 5)# REQUEST THE URL #\n",
    "\n",
    "\n",
    "# Check if page was requested successfully #\n",
    "\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parsing the web page\n",
    "\n",
    "**TASK**: use the `soup()` method to parse the requested web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the contents of the web page from the response\n",
    "\n",
    "soup_response = soup(response.text, \"html.parser\") # Parse the text as a Beautiful Soup object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**QUESTION**: under which tag(s) is the *Registration status history* information found?\n",
    "\n",
    "**TASK**: find the section containing the *Registration status history* information and save it to a variable; view the contents of this variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgdetails = soup_response.find(\"div\", class_=\"field field-name-acnc-node-charity-status-history field-type-ds field-label-hidden\")\n",
    "\n",
    "orgdetails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: change the `orgtable` and `orgdetails` variable names below in order to match your choices from earlier, and execute the code\n",
    "\n",
    "**QUESTION**: explain what the `find_all(\"tr\")` method is doing, and how it fits with the preceeding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgtable = orgdetails.find(\"div\", class_=\"view-content\").find(\"tbody\").find_all(\"tr\")\n",
    "orgtable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Extracting information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: extract organisation status information by inserting code below (HINT: this information is contained in the second column in each row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for row in orgtable:\n",
    "    columns = row.find_all(\"td\") # extract all columns in each row\n",
    "    date = columns[0].text.strip() # organisation status date\n",
    "    status = columns[1].text.strip() # INSERT CODE HERE #\n",
    "    observation = date, status\n",
    "    \n",
    "    varnames = [\"status_date\", \"status\"]\n",
    "    with open(\"aus-charity-details.csv\", \"w\") as f:\n",
    "        writer = csv.writer(f, varnames)\n",
    "        writer.writerow(varnames)\n",
    "        writer.writerow(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Saving results from the scrape\n",
    "\n",
    "**TASK**: list the contents of the folder where you saved the results of the scrape\n",
    "\n",
    "**TASK**: open the CSV file where you saved the results of the scrape &mdash; does it look as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check presence of file in \"downloads\" folder\n",
    "\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file and read (import) its contents\n",
    "\n",
    "with open(\"aus-charity-details.csv\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    \n",
    "print(data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FINAL TASK**: execute the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'name' in globals():\n",
    "    print(\"{}, good effort on working through this practical!\".format(name))\n",
    "else:\n",
    "    print(\"You never told me your name at the beginning but you are still deserving of praise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations for working through this practical, you have now (at least to some degree) conducted a successful web scrape of real data. I'm sure you can imagine the immense potential of this method for collecting frequently updated social data in an automated and reliable manner.\n",
    "\n",
    "If you want to see a more complicated version of this practical, then work through Appendix A below.\n",
    "\n",
    "If you are confident in your abilities so far, then start implementing these techniques on your own web scraping idea by completing the following notebook: *ncrm-web-scraping-practical-own-idea-2021-05-17.ipynb*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a snippet of code from a longer programming script that captures data for Australian charities. While more detailed and complicated than what you've encountered so far, the same web scraping logic is applied. \n",
    "\n",
    "Execute the commands below to produce the results; then see if you can understand what most of the code does &mdash; don't worry if it is a bit imposing, the code represents a lot of time, effort and errors on my part over the past year!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from time import sleep\n",
    "import requests\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions #\n",
    "\n",
    "# Download ACNC web pages of charities \n",
    "\n",
    "def webpage_download(webid, abn, **args):\n",
    "    \"\"\"\n",
    "        Downloads a charity's web page from the ACNC website, which can be parsed at a later date.\n",
    "\n",
    "        Takes two mandatory arguments:\n",
    "            - website id of charity i.e., its unique identifier on the regulator's website\n",
    "            - abn of charity, which is its unique organisational id\n",
    "\n",
    "        Dependencies:\n",
    "            - webid_download | webid_download_from_file \n",
    "\n",
    "        Issues:\n",
    "            - does not deal with cases where a charity has more than one web page (e.g., lots of trustees) [SOLVED]   \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Downloading Australian Charity Web Pages\")\n",
    "    print(\"\\r\")\n",
    "\n",
    "    ddate = dt.now().strftime(\"%Y-%m-%d\") # get today's date\n",
    "\n",
    "    \n",
    "    # Create folders\n",
    "\n",
    "    directories = [\"webpages\", \"logs\"]\n",
    "\n",
    "    for directory in directories:\n",
    "        if not os.path.isdir(directory):\n",
    "            os.mkdir(directory)\n",
    "        else:\n",
    "            #print(\"{} already exists\".format(directory))\n",
    "            continue \n",
    "\n",
    "    \n",
    "    # Request web page\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    webadd = \"https://www.acnc.gov.au/charity/\" + str(webid) + \"?page=0\"\n",
    "    response = session.get(webadd)\n",
    "\n",
    "    # Capture metadata\n",
    "\n",
    "    mdata = dict(response.headers)\n",
    "    mdata[\"webid\"] = str(webid)\n",
    "    mdata[\"abn\"] = str(abn)\n",
    "    mdata[\"url\"] = str(webadd)\n",
    "    \n",
    "    # Parse web page\n",
    "\n",
    "    if response.status_code==200:\n",
    "        html_org = response.text # Get the text elements of the page.\n",
    "        soup_org = soup(html_org, \"html.parser\") # Parse as HTML page\n",
    "\n",
    "\n",
    "        # Find additional pages i.e., when a charity has more than 16 trustees\n",
    "\n",
    "        if soup_org.find(\"li\", class_=\"pager-last\"):\n",
    "            pagination = soup_org.find(\"li\", class_=\"pager-last\").find(\"a\").get(\"href\")\n",
    "            numpages = int(pagination[-1:]) + 1\n",
    "        else:\n",
    "            numpages = 1 \n",
    "\n",
    "\n",
    "        # Save results to file\n",
    "\n",
    "        pagenum = 1\n",
    "        outfile = \"./webpages/aus-charity-\" + str(abn) + \"-page-\" + str(pagenum) + \"-\" + ddate + \".txt\"\n",
    "\n",
    "        try: # potential for encoding issues, therefore need to catch\n",
    "            with open(outfile, \"w\", encoding = \"utf-8\") as f:\n",
    "                f.write(html_org)\n",
    "\n",
    "            print(\"Downloaded web page of charity: {}\".format(abn))    \n",
    "            print(\"\\r\")\n",
    "            print(\"Web page file is here: '{}'\".format(outfile))\n",
    "        except Exception as e:\n",
    "             print(\"Could not write to file (potential encoding issue\")\n",
    "             mdata[\"write_to_file\"] = str(e)  \n",
    "\n",
    "        if numpages > 1: # request the remaining pages\n",
    "                 \n",
    "            for i in range(1, numpages):\n",
    "                webadd = \"https://www.acnc.gov.au/charity/\" + str(webid) + \"?page=\" + str(i)\n",
    "                response = session.get(webadd)\n",
    "\n",
    "                if response.status_code==200:\n",
    "                    html_org = response.text # Get the text elements of the page.\n",
    "                    soup_org = soup(html_org, \"html.parser\") # Parse as HTML page\n",
    "\n",
    "                    \n",
    "                    # Save results to file\n",
    "\n",
    "                    pagenum = i + 1\n",
    "                    outfile = \"./webpages/aus-charity-\" + str(abn) + \"-page-\" + str(pagenum) + \"-\" + ddate + \".txt\"\n",
    "                    \n",
    "                    try: # potential for encoding issues, therefore need to catch\n",
    "                        with open(outfile, \"w\", encoding = \"utf-8\") as f:\n",
    "                            f.write(html_org)\n",
    "\n",
    "                        print(\"Downloaded web page of charity: {}\".format(abn))    \n",
    "                        print(\"\\r\")\n",
    "                        print(\"Web page file is here: '{}'\".format(outfile))\n",
    "                    except Exception as e:\n",
    "                        print(\"Could not write to file (potential encoding issue\")\n",
    "                        mdata[\"write_to_file\"] = str(e)\n",
    "\n",
    "                else:\n",
    "                    print(\"\\r\")\n",
    "                    print(\"Could not download web page of charity: {}\".format(abn))    \n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"\\r\")\n",
    "        print(\"Could not download web page of charity: {}\".format(abn))\n",
    "\n",
    "    sleep(2)    \n",
    "\n",
    "    return mdata    \n",
    "\n",
    "\n",
    "# Download ACNC web pages of charities - from file \n",
    "\n",
    "def webpage_download_from_file(infile, prop, **args):\n",
    "    \"\"\"\n",
    "        Takes a CSV file containing webids for Australian charities and\n",
    "        downloads a charity's web page from the ACNC website, which can be parsed at a later date.\n",
    "\n",
    "        Takes one mandatory and one optional argument:\n",
    "            - CSV file containing a list of abns and webids for Australian charities [mandatory]\n",
    "            - Proportion of charities to download web pages for; default is all (1.0) [optional]\n",
    "\n",
    "        Dependencies:\n",
    "            - webid_download | webid_download_from_file \n",
    "\n",
    "        Issues:\n",
    "            - does not deal with cases where a charity has more than one web page (e.g., lots of trustees) [SOLVED]\n",
    "    \"\"\"\n",
    "\n",
    "    ddate = dt.now().strftime(\"%Y-%m-%d\") # get today's date\n",
    "\n",
    "\n",
    "    # Read in data\n",
    "\n",
    "    df = pd.read_csv(infile, encoding = \"ISO-8859-1\", index_col=False) # import file\n",
    "    prop = float(prop)\n",
    "    df = df.sample(frac=prop) # take random sample (default is to keep all rows in dataframe)\n",
    "\n",
    "    \n",
    "    # Create list and file to store metadata of requests\n",
    "\n",
    "    mfile = \"./logs/aus-webpages-metadata-\" + ddate + \".json\"\n",
    "    mlist = []\n",
    "    \n",
    "\n",
    "    # Request web pages\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        webid = getattr(row, \"webid\")\n",
    "        abn = getattr(row, \"abn\")\n",
    "        mdata = webpage_download(webid, abn)\n",
    "        mlist.append(mdata)\n",
    "\n",
    "     # Write metadata to file\n",
    "\n",
    "    with open(mfile, \"w\") as f:\n",
    "        json.dump(mlist, f) \n",
    "\n",
    "    print(\"\\r\")\n",
    "    print(\"Finished downloading web pages for charities in file: {}\".format(infile))\n",
    "    print(\"Check log file for metadata about the download: {}\".format(mfile))\n",
    "\n",
    "\n",
    "\n",
    "# History Data #\n",
    "\n",
    "def history(source, **args):\n",
    "    \"\"\"\n",
    "        Takes a charity's webpage (.txt file) downloaded from the ACNC website and\n",
    "        extracts the history of the organisation:\n",
    "            - registration\n",
    "            - enforcement\n",
    "            - subtype (i.e., charitable purpose)\n",
    "\n",
    "        Takes one mandatory argument:\n",
    "            - A directory with .txt files containing HTML code of a charity's ACNC web page\n",
    "\n",
    "        Dependencies:\n",
    "            - webpage_download | webpage_download_from_file \n",
    "\n",
    "        Issues:\n",
    "            - duplicates the information for the final charity in the loop  [SOLVED]     \n",
    "    \"\"\"\n",
    "\n",
    "   # Create folders\n",
    "\n",
    "    directories = [\"history\", \"logs\"]\n",
    "\n",
    "    for directory in directories:\n",
    "        if not os.path.isdir(directory):\n",
    "            os.mkdir(directory)\n",
    "        else:\n",
    "            #print(\"{} already exists\".format(directory))\n",
    "            continue \n",
    "\n",
    "    ddate = dt.now().strftime(\"%Y-%m-%d\") # get today's date\n",
    "    \n",
    "\n",
    "    # Define output files\n",
    "\n",
    "    enffile = \"./history/aus-enforcement-\" + ddate + \".csv\"\n",
    "    subfile = \"./history/aus-subtype-\" + ddate + \".csv\"\n",
    "    regfile = \"./history/aus-registration-\" + ddate + \".csv\"\n",
    "    logfile = \"./logs/aus-history-metadata-\" + ddate + \".csv\"\n",
    "\n",
    "   \n",
    "    # Define variable names for the output files\n",
    "    \n",
    "    evarnames = [\"abn\", \"enforcement\", \"enforcement_date\", \"summary\", \"variation\", \"variation_date\", \"report\", \"note\"]\n",
    "    rvarnames = [\"abn\", \"status_date\", \"status\", \"note\"]\n",
    "    svarnames = [\"abn\", \"purpose\", \"start_date\", \"end_date\", \"note\"]\n",
    "    lvarnames = [\"abn\", \"enforcement_history\", \"subtype_history\", \"registration_history\"]\n",
    "\n",
    "\n",
    "    # Write headers to the output files\n",
    "\n",
    "    with open(enffile, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f, evarnames)\n",
    "        writer.writerow(evarnames)\n",
    "\n",
    "    with open(regfile, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f, rvarnames)\n",
    "        writer.writerow(rvarnames)  \n",
    "\n",
    "    with open(subfile, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f, svarnames)\n",
    "        writer.writerow(svarnames)\n",
    "\n",
    "    with open(logfile, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f, lvarnames)\n",
    "        writer.writerow(lvarnames)    \n",
    "    \n",
    "\n",
    "    # Read data\n",
    "\n",
    "    for file in os.listdir(source):\n",
    "        if file.endswith(\".txt\"):\n",
    "            abn = file[12:23] # extract abn from file name\n",
    "            f = os.path.join(source, file)\n",
    "            print(\"Opening {} of charity {}\".format(f, abn))\n",
    "            with open(f, \"r\", encoding = \"utf-8\") as f:\n",
    "                data = f.read()\n",
    "                soup_org = soup(data, \"html.parser\") # Parse the text as a BS object.\n",
    "            \n",
    "\n",
    "            # Extract specific pieces of information: registration, enforcement, charitable purpose\n",
    "\n",
    "            # Enforcement\n",
    "\n",
    "            #print(\"Extracting enforcement history of charity: {}\".format(abn))\n",
    "\n",
    "            enfdetails = soup_org.find(\"div\", class_=\"field field-name-acnc-node-charity-compliance-history field-type-ds field-label-hidden\")      \n",
    "            \"\"\"\n",
    "                Groups have an enforcement section on their webpage; they do not for registration or subtype.\n",
    "            \"\"\"\n",
    "\n",
    "            if enfdetails.find(\"div\", class_=\"view-empty\"): # If there is no enforcement history\n",
    "                enforcement_history = 0\n",
    "                with open(enffile, \"a\", newline=\"\") as f:\n",
    "                    row = abn, \"NULL\", \"NULL\", \"NULL\", \"NULL\", \"NULL\", \"NULL\", \"No enforcement history\"\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(row)\n",
    "\n",
    "            elif enfdetails.find(\"div\", class_=\"view-content\"):\n",
    "                enforcement_history = 1\n",
    "                enfcontent = enfdetails.find(\"div\", class_=\"view-content\")\n",
    "                enftable = enfcontent.find(\"tbody\").find_all(\"tr\")\n",
    "               \n",
    "                for row in enftable:\n",
    "                    td_list = row.find_all(\"td\")\n",
    "                    enftype = td_list[0].text.strip() # Type of enforcement\n",
    "                    enfdate = td_list[1].text.strip() # Date of enforcement\n",
    "                    enfsummary = td_list[2].text.strip() # Text summary of enforcement\n",
    "                    enfvar = td_list[3].text.strip() # Variation in enforcement\n",
    "                    enfvardate = td_list[4].text.strip() # Date of variation in enforcement\n",
    "                    enfrep = td_list[5].find(\"a\").get(\"href\") # Link to enforcement report\n",
    "                    note = \"NULL\"\n",
    "                    row = abn, enftype, enfdate, enfsummary, enfvar, enfvardate, enfrep, note\n",
    "                    print(row)  \n",
    "                    with open(enffile, \"a\", newline=\"\") as f:\n",
    "                        writer = csv.writer(f)\n",
    "                        writer.writerow(row)\n",
    "\n",
    "            else: # Couldn't find enforcement details\n",
    "                enforcement_history = -9\n",
    "                with open(enffile, \"a\", newline=\"\") as f:\n",
    "                    row = abn, \"NULL\", \"NULL\", \"NULL\", \"NULL\", \"NULL\", \"NULL\", \"Could not find enforcement information on web page\"\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(row)\n",
    "\n",
    "            \n",
    "            # Registration and revocation\n",
    "\n",
    "            #print(\"Extracting registration and revocation history of charity: {}\".format(abn))\n",
    "\n",
    "            revdetails = soup_org.find(\"div\", class_=\"field field-name-acnc-node-charity-status-history field-type-ds field-label-hidden\")\n",
    "\n",
    "            if soup_org.find(\"div\", class_=\"group-info field-group-div\"): # If charity is part of a group\n",
    "                registration_history = -8\n",
    "                with open(regfile, \"a\", newline=\"\") as f:\n",
    "                    row = abn, \"NULL \", \"NULL\", \"Group charity\"\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(row)\n",
    "                continue          \n",
    "\n",
    "            elif revdetails.find(\"div\", class_=\"view-empty\"): # If there is no registration history\n",
    "                registration_history = 0\n",
    "                with open(regfile, \"a\", newline=\"\") as f:\n",
    "                    row = abn, \"NULL\", \"NULL\", \"No status information found\"\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(row)\n",
    "\n",
    "            elif revdetails.find(\"div\", class_=\"view-content\"): # if there is a registration history\n",
    "                registration_history = 1\n",
    "                revcontent = revdetails.find(\"div\", class_=\"view-content\")\n",
    "                revtable = revcontent.find(\"tbody\").find_all(\"tr\")\n",
    "                \n",
    "                for row in revtable:\n",
    "                    td_list = row.find_all(\"td\")\n",
    "                    # Get relevant tds and write to output file\n",
    "                    revdate = td_list[0].text.strip() # Effective date\n",
    "                    revstatus = td_list[1].text.strip() # Status\n",
    "                    note = \"NULL\"\n",
    "                    row = abn, revdate, revstatus, note\n",
    "                    with open(regfile, \"a\", newline=\"\") as f:\n",
    "                        writer = csv.writer(f)\n",
    "                        writer.writerow(row)\n",
    "\n",
    "            else: # Could not find registration and revocation details\n",
    "                registration_history = -9\n",
    "                with open(subfile, \"a\", newline=\"\") as f:\n",
    "                    row = abn, \"NULL\", \"NULL\", \"NULL\", \"Could not find registration and revocation information on web page\"\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(row)            \n",
    "\n",
    "\n",
    "            # Purposes\n",
    "\n",
    "            #print(\"Extracting charitable purpose history of charity: {}\".format(abn))\n",
    "\n",
    "            subdetails = soup_org.find(\"div\", class_=\"field field-name-acnc-node-charity-subtype-history field-type-ds field-label-hidden\")\n",
    "\n",
    "            if soup_org.find(\"div\", class_=\"group-info field-group-div\"): # If charity is part of a group\n",
    "                subtype_history = -8\n",
    "                with open(subfile, \"a\", newline=\"\") as f:\n",
    "                    row = abn, \"NULL\", \"NULL\", \"NULL\", \"Group charity\"\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(row)\n",
    "                continue                     \n",
    "\n",
    "            elif subdetails.find(\"div\", class_=\"view-empty\"): # If there is no subtype history\n",
    "                subtype_history = 0\n",
    "                with open(subfile, \"a\", newline=\"\") as f:\n",
    "                    row = abn, \"NULL\", \"NULL\", \"NULL\", \"No subtype history\"\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(row)\n",
    "\n",
    "            elif subdetails.find(\"div\", class_=\"view-content\"):\n",
    "                subtype_history = 1\n",
    "                subcontent = subdetails.find(\"div\", class_=\"view-content\")\n",
    "                subtable = subcontent.find(\"tbody\").find_all(\"tr\")\n",
    "               \n",
    "                for row in subtable:\n",
    "                    td_list = row.find_all(\"td\")\n",
    "                    subtype = td_list[0].text.strip() # Type of purpose\n",
    "                    sdate = td_list[1].text.strip() # Start date of purpose\n",
    "                    edate = td_list[2].text.strip() # End date of purpose\n",
    "                    if edate == \"â€”\":\n",
    "                        edate = edate.replace(\"â€”\", \"NULL\")\n",
    "                    note = \"NULL\"\n",
    "                    row = abn, subtype, sdate, edate, note \n",
    "                    with open(subfile, \"a\", newline=\"\") as f:\n",
    "                        writer = csv.writer(f)\n",
    "                        writer.writerow(row)\n",
    "\n",
    "            else: # Couldn't find purpose details\n",
    "                subtype_history = -9\n",
    "                with open(subfile, \"a\", newline=\"\") as f:\n",
    "                    row = abn, \"NULL\", \"NULL\", \"NULL\", \"Could not find subtype information on web page\"\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(row)\n",
    "\n",
    "    \n",
    "            # Write metadata to logfile\n",
    "\n",
    "            with open(logfile, \"a\", newline=\"\") as f:\n",
    "                row = abn, enforcement_history, subtype_history, registration_history\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(row)\n",
    "\n",
    "    print(\"\\r\")\n",
    "    print(\"Finished extracting history data from charity web pages found in: {}\".format(source))\n",
    "    print(\"Check log file for metadata about the extraction: {}\".format(logfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute functions #\n",
    "\n",
    "webpage_download_from_file(\"./data/aus-webids-master.csv\", prop=.10)\n",
    "history(\"./webpages/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View downloaded data #\n",
    "\n",
    "os.listdir(\"history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registration = pd.read_csv(\"./history/aus-registration-2021-05-15.csv\", index_col = False)\n",
    "registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purposes = pd.read_csv(\"./history/aus-subtype-2021-05-15.csv\", index_col = False)\n",
    "purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enforcement = pd.read_csv(\"./history/aus-enforcement-2021-05-15.csv\", index_col = False)\n",
    "enforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--END OF FILE--"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "256px",
    "width": "221px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
