{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Web scraping using Python: Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Computational methods for collecting, cleaning and analysing data are an increasingly important component of a social scientistâ€™s toolkit. Central to engaging in these methods is the ability to write readable and effective code using a programming language.\n",
    "\n",
    "In this lesson we apply the logic of web scraping to some a simple, genuine website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aims\n",
    "\n",
    "This lesson - **Web scraping using Python: Example 2** - has two aims:\n",
    "1. Demonstrate how to use Python to collect data found on more complicated, realistic websites.\n",
    "2. Cultivate your computational thinking skills through coding examples. In particular, how to define and solve a data collection problem using a computational method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lesson details\n",
    "\n",
    "* **Level**: Introductory\n",
    "* **Time**: 30-40 minutes\n",
    "* **Pre-requisites**: None\n",
    "* **Audience**: Researchers and analysts from any disciplinary background\n",
    "* **Learning outcomes**:\n",
    "    1. Understand the key steps and requirements for collecting data from web pages using computational methods.\n",
    "    2. Be able to use Python for requesting, parsing, extracting and saving data stored on a web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Guide to using this resource\n",
    "\n",
    "This learning resource was built using <a href=\"https://jupyter.org/\" target=_blank>Jupyter Notebook</a>, an open-source software application that allows you to mix code, results and narrative in a single document. As <a href=\"https://jupyter4edu.github.io/jupyter-edu-book/\" target=_blank>Barba et al. (2019)</a> espouse:\n",
    "> In a world where every subject matter can have a data-supported treatment, where computational devices are omnipresent and pervasive, the union of natural language and computation creates compelling communication and learning opportunities.\n",
    "\n",
    "If you are familiar with Jupyter notebooks then skip ahead to the main content (*What is web-scraping?*). Otherwise, the following is a quick guide to navigating and interacting with the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interaction\n",
    "\n",
    "**You only need to execute the code that is contained in sections which are marked by `In []`.**\n",
    "\n",
    "To execute a cell, click or double-click the cell and press the `Run` button on the top toolbar (you can also use the keyboard shortcut Shift + Enter).\n",
    "\n",
    "Try it for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Enter your name and press enter:\")\n",
    "name = input()\n",
    "print(\"\\r\")\n",
    "print(\"Hello {}, enjoy learning more about Python and web-scraping!\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learn more\n",
    "\n",
    "Jupyter notebooks provide rich, flexible features for conducting and documenting your data analysis workflow. To learn more about additional notebook features, we recommend working through some of the <a href=\"https://github.com/darribas/gds19/blob/master/content/labs/lab_00.ipynb\" target=_blank>materials</a> provided by Dani Arribas-Bel at the University of Liverpool. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is the general approach for scraping data from a web page?\n",
    "\n",
    "We begin by identifying a web page containing information we are interested in collecting. Then we need to **know** the following:\n",
    "1. The location (i.e., web address) where the web page can be accessed. For example, the UK Data Service homepage can be accessed via <a href=\"https://ukdataservice.ac.uk/\" target=_blank>https://ukdataservice.ac.uk/</a>.\n",
    "2. The location of the information we are interested in within the structure of the web page. This involves visually inspecting a web page's underlying code using a web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And **do** the following:\n",
    "1. Request the web page using its web address.\n",
    "2. Parse the structure of the web page so your programming language can work with its contents.\n",
    "3. Extract the information we are interested in.\n",
    "4. Write this information to a file for future use.\n",
    "\n",
    "For any programming task, it is useful to write out the steps needed to solve the problem: we call this *pseudo-code*, as it is captures the main tasks and the order in which they need to be executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A social science web scraping example: COVID-19 data\n",
    "\n",
    "The examples we covered previously were ideal for demonstrating the general web scraping approach. However the websites and files were simple to navigate, request and parse. It's time to encounter more difficult examples, this time focusing on accessing up-to-date COVID-19 data from Worldometer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worldometer is a website that provides up-to-date statistics on the following domains: the global population; food, water and energy consumption; environmental degradation and many others (known as its Real Time Statistics Project). In its own words:<sup>[1]</sup>\n",
    "> Worldometer is run by an international team of developers, researchers, and volunteers with the goal of making world statistics available in a thought-provoking and time relevant format to a wide audience around the world. Worldometer is owned by Dadax, an independent company. We have no political, governmental, or corporate affiliation.\n",
    "\n",
    "Since the outbreak of Covid-19 it has provided regular daily snapshots on the progress of this disease, both globally and at a country level.\n",
    "\n",
    "[1]: https://www.worldometers.info/about/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Identifying the web address\n",
    "\n",
    "The website can be accessed here: <a href=\"https://www.worldometers.info/coronavirus/\" target=_blank>https://www.worldometers.info/coronavirus/</a>\n",
    "\n",
    "Let's work through the steps necessary to collect data about the number of Covid-19 cases, deaths and recoveries globally.\n",
    "\n",
    "First, let's become familiar with this website: click on the link below to view the web page in your browser: <a href=\"https://www.worldometers.info/coronavirus/\" target=_blank>https://www.worldometers.info/coronavirus/</a>\n",
    "\n",
    "(Note: it possible to load websites into Python in order to view them, however the website we are interested in doesn't allow this. See the example code below for how it would work for a different website - just remove the quotation marks enclosing the code and run the cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(\"https://httpbin.org/html\", width=\"600\", height=\"650\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Locating information\n",
    "\n",
    "The statistics we need are near the top of the page under the following headings:\n",
    "* Coronavirus Cases:\n",
    "* Deaths:\n",
    "* Recovered:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Visually inspecting the underlying HTML code\n",
    "\n",
    "Therefore, what we need are the tags that identify the section of the web page where the statistics are stored. We can discover the tags by examining the *source code* (HTML) of the web page. This can be done using your web browser: for example, if you use use Firefox you can right-click on the web page and select *View Page Source* from the list of options. \n",
    "\n",
    "The cell below shows a snippet of the source code for the section of the web page we are interested in."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div id=\"maincounter-wrap\" style=\"margin-top:15px\">\n",
    "    <h1>Coronavirus Cases:</h1>\n",
    "    <div class=\"maincounter-number\">\n",
    "        <span style=\"color:#aaa\">252,731        </span>\n",
    "\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"text-align:center \"><a href=\"#countries\">view by country</a></div>\n",
    "\n",
    "<div id=\"maincounter-wrap\" style=\"margin-top:15px\">\n",
    "    <h1>Deaths:</h1>\n",
    "    <div class=\"maincounter-number\">\n",
    "        <span>10,405</span>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "<div id=\"maincounter-wrap\" style=\"margin-top:15px;\">\n",
    "    <h1>Recovered:</h1>\n",
    "    <div class=\"maincounter-number\" style=\"color:#8ACA2B \">\n",
    "        <span>89,056</span>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the above example, we can see multiple tags that contain various elements (e.g., text content, other tags). For instance, we can see that the Covid-19 statistics are enclosed in `<span><\\span>` tags, which themselves are located within `<div><\\div>` tags.\n",
    "\n",
    "As you can see, exploring and locating the contents of a web page remains a manual and visual process, and in Brooker's estimation (2020, 252):\n",
    "> Hence, more so than the actual Python, it's the detective work of unpicking the internal structure of a webpage that is probably the most vital skill here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Requesting the web page\n",
    "\n",
    "Now that we possess the necessary information, let's begin the process of scraping the web page. There is a preliminary step, which is setting up Python with the modules it needs to perform the web-scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "import os # module for navigating your machine (e.g., file directories)\n",
    "import requests # module for requesting urls\n",
    "import csv # module for handling csv files\n",
    "import pandas as pd # module for handling data\n",
    "from datetime import datetime # module for working with dates and time\n",
    "from bs4 import BeautifulSoup as soup # module for parsing web pages\n",
    "\n",
    "print(\"Succesfully imported necessary modules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Modules are additional techniques or functions that are not present when you launch Python. Some do not even come with Python when you download it and must be installed on your machine separately - think of using `ssc install <package>` in Stata, or `install.packages(<package>)` in R. For now just understand that many useful modules need to be imported every time you start a new Python session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now, let's implement the process of scraping the page. First, we need to request the web page using Python; this is analogous to opening a web browser and entering the web address manually. We refer to a page's location on the internet as its web address or Uniform Resource Locator (URL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the URL where the web page can be accessed\n",
    "\n",
    "url = \"https://www.worldometers.info/coronavirus/\"\n",
    "\n",
    "# Request the web page from the URL\n",
    "\n",
    "response = requests.get(url, allow_redirects=True) # request the url\n",
    "# response.headers\n",
    "response.status_code # check if page was requested successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Good, we get a status code of *200*, which means the request was successful. A status code in *400s* or *500s* represent an unsuccessful attempt at requesting a web page (see <a href=\"https://www.textbook.ds100.org/ch/07/web_http.html\" target=_blank>Lau, Gonzalez and Nolan</a> for a succinct description of different types of response status codes).\n",
    "\n",
    "Let's unpack the code a bit. First, we define a variable (also known as an 'object' in Python) called `url` that contains the web address of the page we want to request. Next, we use the `get()` method of the `requests` module to request the web page, and in the same line of code, we store the results of the request in a variable called `response`. Finally, we check whether the request was successful by calling on the `status_code` attribute of the `response` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confused? Don't worry, the conventions of Python and using its modules take a bit of getting used to. At this point, just understand that you can store the results of commands in variables, and a variable can have different attributes that can be accessed when needed. Also note that you have a lot of freedom in how you name your variables (subject to certain restrictions - see <a href=\"https://www.python.org/dev/peps/pep-0008/\" target=_blank>here for some guidance</a>).\n",
    "\n",
    "For example, the following would also work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_address = \"https://www.worldometers.info/coronavirus/\"\n",
    "\n",
    "scrape_result = requests.get(web_address, allow_redirects=True)\n",
    "scrape_result.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also view the metadata associated with our request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "response.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Back to the request:\n",
    "\n",
    "Good, we get a status code of _200_ - this means we successfully requested the web page. <a href=\"https://www.textbook.ds100.org/ch/07/web_http.html\" target=_blank>Lau, Gonzalez and Nolan</a> provide a succinct description of different types of response status codes:\n",
    "\n",
    "* **100s** - Informational: More input is expected from client or server (e.g. 100 Continue, 102 Processing)\n",
    "* **200s** - Success: The client's request was successful (e.g. 200 OK, 202 Accepted)\n",
    "* **300s** - Redirection: Requested URL is located elsewhere; May need user's further action (e.g. 300 Multiple Choices, 301 Moved Permanently)\n",
    "* **400s** - Client Error: Client-side error (e.g. 400 Bad Request, 403 Forbidden, 404 Not Found)\n",
    "* **500s** - Server Error: Server-side error or server is incapable of performing the request (e.g. 500 Internal Server Error, 503 Service Unavailable)\n",
    "\n",
    "For clarity:\n",
    "* **Client**: your machine\n",
    "* **Server**: the machine you are requesting the web page from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be wondering exactly what it is we requested: if you were to type the URL (https://www.worldometers.info/coronavirus/) into your browser and hit `enter`, the web page should appear on your screen. This is not the case when we request the URL through Python but rest assured, we have successfully requested the web page. To see the content of our request, we can examine the `text` attribute of the `response` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "response.text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This shows us the underlying code (HTML) of the web page we requested. It should be obvious that in its current form, the result of this request will be difficult to work with. This is where the `BeautifulSoup` module comes in handy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parsing the web page\n",
    "\n",
    "Now it's time to identify and understand the structure of the web page we requested. We do this by converting the content contained in the `response.text` attribute into a `BeautifulSoup` variable. `BeautifulSoup` is a Python module that provides a systematic way of navigating the elements of a web page and extracting its contents. Let's see how it works in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the contents of the web page from the response\n",
    "\n",
    "soup_response = soup(response.text, \"html.parser\") # Parse the text as a Beautiful Soup object\n",
    "soup_sample = soup(response.text[:1000], \"html.parser\") # Parse a sample of the text\n",
    "soup_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice how the hierarchical structure of the web page is now recognised by Python? `BeautifulSoup` has taken the unstructured text contained in `response.text` and parsed it as HTML: now we can clearly see the hierarchical structure and tags that comprise a web page's HTML. \n",
    "\n",
    "Note again how we call on a method (`soup()`) from a module (`BeautifulSoup`) and store the results in a variable (`soup_response`).\n",
    "\n",
    "Of course, we've only displayed a sample of the code here for readability. What about the full text contained in `soup_response`: how do we navigate such voluminous results? Thankfully the `BeautifulSoup` module provides some intuitive methods for doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Extracting information\n",
    "\n",
    "Now that we have parsed the web page, we can use Python to navigate and extract the information of interest. To begin with, let's locate the section of the web page containing the overall Covid-19 statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sections = soup_response.find_all(\"div\", id=\"maincounter-wrap\")\n",
    "sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We used the `find_all()` method to search for all `<div>` tags where the id=\"maincounter-wrap\". And because there is more than one set of tags matching this id, we get a list of results. We can check how many tags match this id by calling on the `len()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can view each element in the list of results as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for section in sections:\n",
    "    print(\"--------\")\n",
    "    print(section)\n",
    "    print(\"--------\")\n",
    "    print(\"\\r\") # print some blank space for better formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are nearing the end of our scrape. The penultimate task is to extract the statistics within the `<span>` tags and store them in some variables. We do this by accessing each item in the _sections_ list using its positional value (index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cases = sections[0].find(\"span\").text.replace(\" \", \"\").replace(\",\", \"\")\n",
    "deaths = sections[1].find(\"span\").text.replace(\",\", \"\")\n",
    "recov = sections[2].find(\"span\").text.replace(\",\", \"\")\n",
    "print(\"Number of cases: {}; deaths: {}; and recoveries: {}.\".format(cases, deaths, recov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The above code performs a couple of operations:\n",
    "* For each item (i.e., set of `<div>` tags) in the list, it finds the `<span>` tags and extracts the text enclosed within them.\n",
    "* We clean the text by removing blank spaces and commas.\n",
    "\n",
    "In this example, referring to an item's positional index works because our list of `<div>` tags stored in the `sections` variable is ordered: the tag containing the number of cases appears before the tag containing the number of deaths, which appears before the tag containing the number of recovered patients.\n",
    "\n",
    "In Python, indexing begins at zero (in R indexing begins at 1). Therefore, the first item in the list is accessed using `sections[0]`, the second using `sections[1]` etc.\n",
    "\n",
    "(To learn more about lists in Python, see Chapter 22 of <a href=\"https://assets.digitalocean.com/books/python/how-to-code-in-python.pdf\" target=_blank>*How to Code in Python 3*</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Saving results from the scrape\n",
    "\n",
    "The final task is to save the variables to a file that we can use in the future. We'll write to a Comma-Separated Values (CSV) file for this purpose, as it is an open-source, text-based file format that is commonly used for sharing data on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a downloads folder\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"./downloads\")\n",
    "except:\n",
    "    print(\"Unable to create folder: already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of \"./\" tells the `os.mkdir()` command that the \"downloads\" folder should be created at the same level of the directory where this notebook is located. So if this notebook was stored in a directory located at \"C:/Users/joebloggs/notebooks\", the `os.mkdir()` command would result in a new folder located at \"C:/Users/joebloggs/notebooks/downloads\".\n",
    "   \n",
    "(Technically the \"./\" is not needed and you could just write `os.mkdir(\"downloads\")` but it's good practice to be explicit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Write the results to a CSV file\n",
    "\n",
    "date = datetime.now().strftime(\"%Y-%m-%d\") # get today's date in YYYY-MM-DD format\n",
    "print(date)\n",
    "\n",
    "variables = [\"Cases\", \"Deaths\", \"Recoveries\"] # define variable names for the file\n",
    "outfile = \"./downloads/covid-19-statistics-\" + date + \".csv\" # define a file for writing the results\n",
    "obs = cases, deaths, recov # define an observation (row)\n",
    "print(obs)\n",
    "\n",
    "with open(outfile, \"w\", newline=\"\") as f: # with the file open in \"write\" mode, and giving it a shorter name (f)\n",
    "    writer = csv.writer(f) # define a 'writer' object that allows us to export information to a CSV\n",
    "    writer.writerow(variables) # write the variable names to the first row of the file\n",
    "    writer.writerow(obs) # write the observation to the next row in the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above defines some headers and a name and location for the file which will store the results of the scrape. We then open the file in *write* mode, and write the headers to the first row, and the statistics to subsequent rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know this worked? The simplest way is to check whether a) the file was created, and b) the results were written to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check presence of file in \"downloads\" folder\n",
    "\n",
    "os.listdir(\"./downloads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file and read (import) its contents\n",
    "\n",
    "with open(outfile, \"r\") as f:\n",
    "    data = f.read()\n",
    "    \n",
    "print(data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And Voila, we have successfully scraped a web page!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country-level COVID-19 data\n",
    "\n",
    "We will complete our work gathering data on the COVID-19 pandemic by employing the techniques we learned previously to capture country-level statistics. We'll assume some knowledge on your part and thus you'll notice fewer annotations explaining what is happening at each step. As we progress through this example, there are some tasks for you to complete and some questions to be answered also.\n",
    "\n",
    "**TASK**: if you need to, re-acquaint yourself with the <a href=\"https://www.worldometers.info/coronavirus/\" target=_blank>Worldometer website</a> - the table with country-level data is located near the bottom of the page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get some the preliminaries out of the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "date = datetime.now().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request the web page and parse it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.worldometers.info/coronavirus/\"\n",
    "\n",
    "response = requests.get(url, allow_redirects=True)\n",
    "response.headers\n",
    "soup_response = soup(response.text, \"html.parser\")\n",
    "#\n",
    "# QUESTION: How do you know if the web page was requested successfully?\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the table containing country-level statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup_response.find(\"table\", id=\"main_table_countries_today\").find(\"tbody\")\n",
    "rows = table.find_all(\"tr\", style=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the information contained in each row in the table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_info = []\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    country_info = [column.text.strip() for column in columns]\n",
    "    del country_info[7]\n",
    "    global_info.append(country_info)\n",
    "\n",
    "print(global_info[0:10])\n",
    "print(\"\\r\")\n",
    "print(\"Number of rows in table: {}\".format(len(global_info)))\n",
    "print(\"\\r\")\n",
    "\n",
    "del global_info[0] # delete first row containing world statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a blank list to store statistics for each country (`global_info = []`); then for each row in the table, we extract the contents of each column and store the results in a list (`country_info = [column.text.strip() for column in columns])`; finally we add the results for each country to the overall list (`global_info.append(country_info)`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the results of the scrape to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(\"./downloads\")\n",
    "except OSError as error:\n",
    "    print(\"Folder already exists\")\n",
    "\n",
    "variables = [\"Number\", \"Country\", \"Total Cases\", \"New Cases\", \"Total Deaths\", \n",
    "            \"New Deaths\", \"Total Recovered\", \"Active Cases\", \n",
    "            \"Serious_Critical\", \"Total Cases Per 1m Pop\", \"Deaths Per 1m Pop\",\n",
    "            \"Total Tests\", \"Tests Per 1m Pop\", \"Population\"]\n",
    "outfile = \"./downloads/covid-19-country-statistics-\" + date + \".csv\"\n",
    "print(outfile)\n",
    "\n",
    "with open(outfile, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(variables)\n",
    "    for country in global_info:\n",
    "        writer.writerow(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check the file was created; if so we load it into Python and examine its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(outfile, encoding = \"ISO-8859-1\", index_col=False)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data[\"Country\"]==\"Ireland\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What have we learned?\n",
    "\n",
    "Let's recap what key skills and techniques we've learned:\n",
    "* **How to import modules**. You will usually need to import modules into Python to support your work. Python does come with some methods and functions that are ready to use straight away, but for computational social science tasks you'll almost certainly need to import some additional modules.\n",
    "* **How to request and parse web pages**. You can use Python to request a web page, and the `BeautifulSoup` module to parse its contents.\n",
    "* **How to read and write data**. You can save the results of your scrape to a file for future use.\n",
    "* **How to do all of the above in an efficient, clear and effective manner**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "While the above examples demonstrate the basics of web scraping well, collecting research-relevant data from a web page is a little more difficult:\n",
    "* Data may be spread throughout a web page (or across multiple pages).\n",
    "* There may be many tags with similar data that need to be filtered in order to get to the information you need.\n",
    "* And many other potential issues.\n",
    "\n",
    "Thankfully the process/logic is the same even for more complicated examples - we'll explore these in the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Bibliography\n",
    "\n",
    "Barba, Lorena A. et al. (2019). *Teaching and Learning with Jupyter*. <a href=\"https://jupyter4edu.github.io/jupyter-edu-book/\" target=_blank>https://jupyter4edu.github.io/jupyter-edu-book/</a>.\n",
    "\n",
    "Lau, S., Gonzalez, J., & Nolan, D. (n.d.). *Principles and Techniques of Data Science*. https://www.textbook.ds100.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "--END OF FILE--"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "256px",
    "width": "221px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
